{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-02 15:08:31.135027: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2024-01-02 15:08:31.135064: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tensorflow.io import gfile\n",
    "import soundfile as sf\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
    "from tqdm import *\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Ourclassifer(nn.Module):\n",
    "    def __init__(self, in_dim, n_hidden1, n_hidden2, out_dim=1, drop_prob1=0.5, drop_prob2=0.5):\n",
    "        super(Ourclassifer, self).__init__()\n",
    "        self.layer1 = nn.Linear(in_dim, n_hidden1)\n",
    "        self.layer2 = nn.Linear(n_hidden1, n_hidden2)\n",
    "        self.layer3 = nn.Linear(n_hidden2, out_dim)\n",
    "\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.relu2 = nn.ReLU()\n",
    "\n",
    "        self.drop1 = nn.Dropout(drop_prob1)\n",
    "        self.drop2 = nn.Dropout(drop_prob2)\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.device = torch.device(\"cuda\") if torch.cuda.is_available else torch.device(\"cpu\")\n",
    "        self.to(self.device)\n",
    "    def forward(self, x):\n",
    "        x = x.sum(dim=1)\n",
    "        hidden_1_out = self.relu1(self.layer1(x))\n",
    "        hidden_1_out = self.drop1(hidden_1_out)\n",
    "\n",
    "        hidden_2_out = self.relu2(self.layer2(hidden_1_out))\n",
    "        hidden_2_out = self.drop2(hidden_2_out)\n",
    "        out = self.layer3(hidden_2_out)\n",
    "        out = self.sigmoid(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:1913: FutureWarning: The method `freeze_feature_extractor` is deprecated and will be removed in Transformers v5. Please use the equivalent `freeze_feature_encoder` method instead.\n",
      "  warnings.warn(\n",
      "Some weights of ExtendedWav2Vec2ForCTC were not initialized from the model checkpoint at mandarin-wav2vec2-aishell1 and are newly initialized: ['lm_head.1.layer2.weight', 'lm_head.1.layer1.weight', 'myhead.layer2.weight', 'lm_head.1.layer1.bias', 'myhead.layer1.weight', 'lm_head.1.layer3.weight', 'myhead.layer2.bias', 'myhead.layer3.weight', 'myhead.layer3.bias', 'lm_head.1.layer2.bias', 'myhead.layer1.bias', 'lm_head.1.layer3.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "class ExtendedWav2Vec2ForCTC(Wav2Vec2ForCTC):\n",
    "    \"\"\"\n",
    "    In ESPNET there is a LayerNorm layer between encoder output and CTC classification head.\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.myhead = Ourclassifer(in_dim=config.hidden_size, n_hidden1=config.hidden_size, n_hidden2=config.hidden_size)\n",
    "        self.freeze_feature_extractor()\n",
    "        self.freeze_base_model()\n",
    "        self.lm_head = torch.nn.Sequential(\n",
    "                torch.nn.LayerNorm(config.hidden_size),\n",
    "                self.myhead\n",
    "        )\n",
    "        for param in self.lm_head.parameters():\n",
    "            param.requires_grad = True\n",
    "        self.to(self.myhead.device)\n",
    "        \n",
    "model = ExtendedWav2Vec2ForCTC.from_pretrained(\"mandarin-wav2vec2-aishell1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignored unknown kwarg option normalize\n",
      "Ignored unknown kwarg option normalize\n",
      "Ignored unknown kwarg option normalize\n",
      "Ignored unknown kwarg option normalize\n"
     ]
    }
   ],
   "source": [
    "processor = Wav2Vec2Processor.from_pretrained(\"mandarin-wav2vec2-aishell1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "lr = 0.00001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "ts_writer = SummaryWriter(\"logs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPEECH_DATA_DIR = \"record\"\n",
    "\n",
    "class MyDataSet(Dataset):\n",
    "    def __init__(self, data):\n",
    "        #定义好 image 的路径\n",
    "        self.data = data\n",
    "    def __getitem__(self, index):\n",
    "        file_name = self.data[index][0].decode(\"utf-8\")\n",
    "        #print(file_name)\n",
    "        audio_input, sample_rate = sf.read(file_name)\n",
    "        inputs = processor(audio_input, sampling_rate=sample_rate, return_tensors=\"pt\") \n",
    "        inputs['input_values'] = inputs['input_values'].to(model.device)       \n",
    "        logits = model(**inputs).logits\n",
    "        \n",
    "        logits = logits.squeeze(dim=-1)\n",
    "        return logits, torch.tensor(self.data[index][1]).float()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "def get_label(word):\n",
    "    if word == \"网易精灵\":\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "def get_files(word):\n",
    "    return os.listdir(SPEECH_DATA_DIR + '/' + word + '/')\n",
    "\n",
    "def process_files(file_names, label, repeat):\n",
    "    file_names = tf.repeat(file_names, repeat).numpy()\n",
    "    return [(file_name, label) for file_name in tqdm(file_names, desc=f\"({word}, {label})\", leave=False)]\n",
    "\n",
    "def train(net, num_epochs, train_iter, val_iter, save_dir):\n",
    "    patience = 0\n",
    "    start_epoch = 0\n",
    "    verbose = False  \n",
    "    best_val_loss = 1e10  \n",
    "    earlyStop = False\n",
    "    max_patience = 5\n",
    "    best_train_epochs = None\n",
    "    for i in tqdm(range(start_epoch, num_epochs), disable=not verbose):\n",
    "        net.train()\n",
    "        train_loss = 0.0\n",
    "        cnt = 0\n",
    "        for logits, label in train_iter:\n",
    "            logits = logits.to(net.device)\n",
    "            logits = logits.squeeze(dim=-1)\n",
    "            label = label.to(net.device)\n",
    "\n",
    "            loss = criterion(logits, label)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            cnt += 1\n",
    "        train_loss /= cnt\n",
    "    \n",
    "        ts_writer.add_scalar(\"Train_Loss\", train_loss, i+1)\n",
    "\n",
    "        net.eval()\n",
    "        val_loss = 0.0\n",
    "        cnt = 0\n",
    "        for logits, label in val_iter:\n",
    "            logits = logits.to(net.device)\n",
    "            logits = logits.squeeze(dim=-1)\n",
    "            label = label.to(net.device)\n",
    "            \n",
    "            loss = criterion(logits, label)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            val_loss += loss.item()\n",
    "            cnt += 1\n",
    "        val_loss /= cnt  \n",
    "\n",
    "        ts_writer.add_scalar(\"Val_Loss\", val_loss, i+1) \n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            patience = 0\n",
    "            best_val_loss = val_loss\n",
    "        else:\n",
    "            patience += 1\n",
    "            if patience == max_patience:\n",
    "                earlyStop = True\n",
    "        print(\"epoch: \", i+1, \" train_loss: \", train_loss, \" val_loss: \", val_loss)\n",
    "        if earlyStop:\n",
    "            print(\"Num of training epochs: \", i + 1)\n",
    "            best_train_epochs = i + 1\n",
    "            break\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    torch.save(net.state_dict(), os.path.join(save_dir + f\"/experimental_wakeup_model.pth\"))    \n",
    "    return best_train_epochs\n",
    "\n",
    "def train_all(net, num_epochs, train_iter, save_dir):\n",
    "    start_epoch = 0\n",
    "    verbose = False  \n",
    "\n",
    "    for i in tqdm(range(start_epoch, num_epochs), disable=not verbose):\n",
    "        net.train()\n",
    "        train_loss = 0.0\n",
    "        cnt = 0\n",
    "        for logits, label in train_iter:\n",
    "            logits = logits.to(net.device)\n",
    "            logits = logits.squeeze(dim=-1)\n",
    "            label = label.to(net.device)\n",
    "\n",
    "            loss = criterion(logits, label)            \n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            cnt += 1\n",
    "        train_loss /= cnt\n",
    "    \n",
    "        ts_writer.add_scalar(\"Train_All_Loss\", train_loss, i+1)\n",
    "\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    torch.save(net.state_dict(), os.path.join(save_dir + f\"/final_wakeup_model.pth\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-02 15:09:00.492054: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1\n",
      "2024-01-02 15:09:00.495471: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \n",
      "pciBusID: 0000:10:00.0 name: NVIDIA GeForce RTX 3060 computeCapability: 8.6\n",
      "coreClock: 1.777GHz coreCount: 28 deviceMemorySize: 11.76GiB deviceMemoryBandwidth: 335.32GiB/s\n",
      "2024-01-02 15:09:00.495620: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2024-01-02 15:09:00.495789: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcublas.so.10'; dlerror: libcublas.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2024-01-02 15:09:00.518271: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\n",
      "2024-01-02 15:09:00.524379: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\n",
      "2024-01-02 15:09:00.524677: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcusolver.so.10'; dlerror: libcusolver.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2024-01-02 15:09:00.524862: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcusparse.so.10'; dlerror: libcusparse.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2024-01-02 15:09:00.525036: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudnn.so.7'; dlerror: libcudnn.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2024-01-02 15:09:00.525056: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1753] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2024-01-02 15:09:00.528237: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-02 15:09:00.568876: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2399785000 Hz\n",
      "2024-01-02 15:09:00.574128: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55b9453f2360 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2024-01-02 15:09:00.574151: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2024-01-02 15:09:00.575714: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2024-01-02 15:09:00.575737: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      \n",
      "                                                                     \r"
     ]
    }
   ],
   "source": [
    "train_data = []\n",
    "validate_data = []\n",
    "\n",
    "words =[\n",
    "    '网易精灵',\n",
    "    '网易',\n",
    "    '精灵',\n",
    "    '精力',\n",
    "    '易精',\n",
    "    '网易精',\n",
    "    '易精灵',\n",
    "    '网易精力',\n",
    "    '网红精灵',\n",
    "    '网',\n",
    "    '易',\n",
    "    '精',\n",
    "    '灵',\n",
    "    \"开始\",\n",
    "    \"开机\",\n",
    "    \"停止\",\n",
    "    \"哇\",\n",
    "    \"啊\",\n",
    "    \"_background_noise_\",\n",
    "    \"_silence_\"\n",
    "]\n",
    "\n",
    "SPEECH_DATA_DIR = \"record\"\n",
    "\n",
    "TRAIN_SIZE=0.8\n",
    "VALIDATION_SIZE=0.2\n",
    "\n",
    "\n",
    "for word in words:\n",
    "    file_names = [SPEECH_DATA_DIR + '/' + word + '/' + file_name for file_name in tqdm(get_files(word), leave=False)]\n",
    "    np.random.shuffle(file_names)\n",
    "    train_size = int(TRAIN_SIZE*len(file_names))\n",
    "    validation_size = int(VALIDATION_SIZE*len(file_names))\n",
    "    repeat = 8 if word == \"网易精灵\" else 1  \n",
    "    train_data.extend(process_files(file_names[:train_size], label=get_label(word), repeat=repeat))\n",
    "    validate_data.extend(process_files(file_names[train_size:train_size+validation_size], label=get_label(word), repeat=repeat))\n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "trainDataSet = MyDataSet(data=train_data)\n",
    "valDataSet = MyDataSet(data=validate_data)\n",
    "train_loader = DataLoader(dataset=trainDataSet, batch_size=batch_size)    \n",
    "val_loader = DataLoader(dataset=valDataSet, batch_size=batch_size)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  1  train_loss:  11.179279584792411  val_loss:  6.6702398130948435\n",
      "epoch:  2  train_loss:  3.6330788066062896  val_loss:  1.0358070207161816\n",
      "epoch:  3  train_loss:  2.9958647943239316  val_loss:  0.39578291131278304\n",
      "epoch:  4  train_loss:  2.459462216969098  val_loss:  0.38177907094724256\n",
      "epoch:  5  train_loss:  1.4696236295320169  val_loss:  0.12973300666070492\n",
      "epoch:  6  train_loss:  1.5947255682869863  val_loss:  0.029790134056386818\n",
      "epoch:  7  train_loss:  1.2291408737660676  val_loss:  0.024054903646427376\n",
      "epoch:  8  train_loss:  1.2385043077836984  val_loss:  0.011721716984826927\n",
      "epoch:  9  train_loss:  0.9514352128513472  val_loss:  0.003230450734620973\n",
      "epoch:  10  train_loss:  0.8559534061935403  val_loss:  0.0018150417398808205\n",
      "epoch:  11  train_loss:  0.7003850013364815  val_loss:  0.0032250569668573893\n",
      "epoch:  12  train_loss:  0.5907830977443631  val_loss:  0.0016268307305477947\n",
      "epoch:  13  train_loss:  0.5406827508922911  val_loss:  0.0013760174420074022\n",
      "epoch:  14  train_loss:  0.43367373029209494  val_loss:  0.00013981926463895177\n",
      "epoch:  15  train_loss:  0.36833685742946465  val_loss:  0.0004176637754472523\n",
      "epoch:  16  train_loss:  0.33126743538080744  val_loss:  0.00016844706578638123\n",
      "epoch:  17  train_loss:  0.2760549980219945  val_loss:  0.0002332486594556915\n",
      "epoch:  18  train_loss:  0.2726774758703518  val_loss:  0.000194634312563835\n",
      "epoch:  19  train_loss:  0.2821012638117266  val_loss:  0.00013407280615187033\n",
      "epoch:  20  train_loss:  0.22071843464956561  val_loss:  0.0002488144554166638\n",
      "epoch:  21  train_loss:  0.19351439844000523  val_loss:  0.0001990079463286154\n",
      "epoch:  22  train_loss:  0.1901428235683838  val_loss:  0.00020272430776199\n",
      "epoch:  23  train_loss:  0.19949603118747283  val_loss:  0.00021571186634439243\n",
      "epoch:  24  train_loss:  0.18496129904187308  val_loss:  0.0002052331625392867\n",
      "Num of training epochs:  24\n"
     ]
    }
   ],
   "source": [
    "model_save_path = \"saved_model\"\n",
    "best_train_epochs = train(net=model, num_epochs=100, train_iter=train_loader, val_iter=val_loader, save_dir=model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc:  1.0\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "cnt = 0\n",
    "valid_cnt = 0\n",
    "test_loader = DataLoader(dataset=valDataSet, batch_size=len(validate_data)) \n",
    "for logits, label in test_loader:\n",
    "    logits = logits.to(model.device)\n",
    "    logits = logits.squeeze(dim=-1)\n",
    "    logits = (logits > 0.5).float()\n",
    "    label = label.to(model.device)\n",
    "    cnt += label.shape[0]\n",
    "    valid_cnt += (label == logits).sum().item()\n",
    "    \n",
    "    \n",
    "print(\"acc: \", valid_cnt / cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ExtendedWav2Vec2ForCTC were not initialized from the model checkpoint at mandarin-wav2vec2-aishell1 and are newly initialized: ['lm_head.1.layer3.weight', 'lm_head.1.layer2.bias', 'myhead.layer2.bias', 'lm_head.1.layer3.bias', 'lm_head.1.layer1.bias', 'lm_head.1.layer1.weight', 'myhead.layer3.bias', 'myhead.layer1.bias', 'lm_head.1.layer2.weight', 'myhead.layer3.weight', 'myhead.layer1.weight', 'myhead.layer2.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "all_data = train_data + validate_data\n",
    "allDataSet = MyDataSet(data=all_data)\n",
    "all_loader = DataLoader(dataset=allDataSet, batch_size=batch_size)    \n",
    "\n",
    "model = ExtendedWav2Vec2ForCTC.from_pretrained(\"mandarin-wav2vec2-aishell1\")\n",
    "\n",
    "train_all(model, num_epochs=best_train_epochs, train_iter=all_loader, save_dir=model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignored unknown kwarg option normalize\n",
      "Ignored unknown kwarg option normalize\n",
      "Ignored unknown kwarg option normalize\n",
      "Ignored unknown kwarg option normalize\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ExtendedWav2Vec2ForCTC were not initialized from the model checkpoint at mandarin-wav2vec2-aishell1 and are newly initialized: ['lm_head.1.layer2.weight', 'lm_head.1.layer1.weight', 'myhead.layer2.weight', 'lm_head.1.layer1.bias', 'myhead.layer1.weight', 'lm_head.1.layer3.weight', 'myhead.layer2.bias', 'myhead.layer3.weight', 'myhead.layer3.bias', 'lm_head.1.layer2.bias', 'myhead.layer1.bias', 'lm_head.1.layer3.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8994930386543274\n",
      "我被唤醒啦！\n"
     ]
    }
   ],
   "source": [
    "def get_voice_position(audio, noise_floor):\n",
    "    audio = audio - np.mean(audio)\n",
    "    audio = audio / np.max(np.abs(audio))\n",
    "    return trim(audio, axis=0, epsilon=noise_floor)\n",
    "\n",
    "import wave\n",
    "#import torchaudio\n",
    "import tensorflow_io as tfio\n",
    "from tensorflow_io.core.python.experimental.audio_ops import trim\n",
    "model_save_path = \"saved_model\"\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"mandarin-wav2vec2-aishell1\")\n",
    "model = ExtendedWav2Vec2ForCTC.from_pretrained(\"mandarin-wav2vec2-aishell1\")\n",
    "model.load_state_dict(torch.load(os.path.join(model_save_path + f\"/final_wakeup_model.pth\")))\n",
    "model.eval()\n",
    "sample_rate = 16000\n",
    "NOISE_FLOOR = 0.3\n",
    "\n",
    "file_path = \"record/网易精灵/output_3.wav\"\n",
    "audio_tensor = tfio.audio.AudioIOTensor(file_path)\n",
    "audio = tf.cast(audio_tensor[:], tf.float32)\n",
    "voice_start, voice_end = get_voice_position(audio, NOISE_FLOOR)\n",
    "voice_start = voice_start.numpy()[0]\n",
    "voice_end = voice_end.numpy()[0]\n",
    "\n",
    "valid_audio = audio_tensor[voice_start:voice_end].numpy().reshape(-1).astype(np.float32)\n",
    "\n",
    "inputs = processor(valid_audio, sampling_rate=sample_rate, return_tensors=\"pt\") \n",
    "inputs['input_values'] = inputs['input_values'].to(model.device)       \n",
    "logits = model(**inputs).logits\n",
    "print(logits.item())\n",
    "if logits.item() > 0.5:\n",
    "    print(\"我被唤醒啦！\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
